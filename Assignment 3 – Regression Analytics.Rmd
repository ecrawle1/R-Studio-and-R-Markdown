---
title: "Assignment 3 â€“ Regression Analytics"
author: "Elizabeth Crawley"
date: "2025-06-30"
output: word_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```
##1. Run the following code in R-studio to create two variables X and Y.
```{r}
set.seed(2017)
X=runif(100)*10
Y=X*4+3.45
Y=rnorm(100)*0.29*Y+Y
```
##a) Plot Y against X. Include a screenshot of the plot in your submission. Using the File menu you can save the graph as a picture on your computer. Based on the plot do you think we can fit a linear model to explain Y based on X? (8% of total points)

```{r}
# This will make the random numbers reproducible
set.seed(2017)

# This will create 100 random values between 0 and 10
X <- runif(100) * 10
head(X)  # added this to see the first few values of X

# Calculate Y as a linear function of X: Y = 4*X + 3.45
Y_linear <- 4 * X + 3.45
head(Y_linear)  # see the first few lines of Y_linear

# Add noise to Y:
# rnorm(100) gives 100 values from N(0, 1)
# These are scaled by 0.29 * Y_linear to make noise proportional to Y
noise <- rnorm(100) * 0.29 * Y_linear
head(noise)  # first few noise values

# Final Y is the linear Y plus the noise Y+Y
Y <- Y_linear + noise
head(Y)  # Show final Y values

# Plot (scatterplot) Y against X to visualize the relationship
plot(X, Y, main = "Scatterplot of Y vs X",
     xlab = "X", ylab = "Y",
     pch = 19, col = "blue") # add a pch value of 19 to create a large, solid circle in blue for appearance.

# Fit a linear model and add the regression line to the plot
model <- lm(Y ~ X)
abline(model, col = "red", lwd = 2) # add a lwd value of 2 to make it a medium line again for looks.

# Optional: Summary of the linear model
summary(model)
```
```{r}
# Based on the plot:Yes, a linear model can be used to explain Y based on X. The blue circles, or points on the scatterplot form an obvious line. There are some points spread out which I think is because of the added noise. However, the trend is clearly upward and consistent so using a linear model fits. The red regression line fits the data.
```

##b) Construct a simple linear model of Y based on X. Write the equation that explains Y based on X. What is the accuracy of this model? (8% of total points)
```{r}
##The equation from the output of the simulated data:
##ð‘Œ^ = 4.4565 + 3.6108 â‹… ð‘‹Y^
## = 4.465 5 + 3.6108 â‹… X
##What it means:
##The intercept is approximately 4.656  (This is the predicted value of Y when X = 0)
##The slope is approximately 3.611 (This means that for each 1-unit increase in X, the predicted value of Y increases by approximately 3.61 units)
##r-squared = 0.6517 (meaning around 65.17% of the variation in Y is explained by X - it's not 100% but it's still a good indicator especially with the added noise)
## residual standard error = 7.756 (this is the typical size of the prediction errors)
## p-value for X < 2.2e-16 (meaning the relationship between X and Y is statistically significant [2.2e-16 is scientific notation â€” it means: 2.2 Ã— 10 (power of) âˆ’16 = 0.00000000000000022 ])
```
```{r}
model <- lm(Y ~ X)

summary(model)

## create a visual to make checking my results easier.
plot(model$fitted.values, model$residuals,
     main = "Residuals vs Fitted Values",
     xlab = "Fitted Values",
     ylab = "Residuals",
     pch = 19, col = "darkgreen")
abline(h = 0, col = "red", lwd = 2)
```
##c) How the Coefficient of Determination, R2, of the model above is related to the correlation coefficient of X and Y? (8% of total points)
```{r}
## R2 or the Coefficient of Determination is found in the model here: ##r-squared = 0.6517 and the correlation coefficient (Pearson correlation coefficient) of X and Y is the square of r  0.6517 = 0.8073
## around 0.81 is a strong linear correlation for X and Y
## The regression model shows that X predicts around 65.17% of the variance for Y 

## R2 is the coefficient of determination and it tells us how much of the variation of Y is explained by X
##r is the Pearson correlation coefficient between X and Y and it shows us the direction and strenght of the linear relationship 
## We can square r to find R2 or we can take the square root of R2 to get r.
```
```{r}
set.seed(2017)
X <- runif(100) * 10
Y <- X * 4 + 3.45
Y <- rnorm(100) * 0.29 * Y + Y

model <- lm(Y ~ X)

## Find R2 (R squared) from the model
model_summary <- summary(model)
r_squared_from_model <- model_summary$r.squared
cat("R-squared from model:", r_squared_from_model, "\n")

## Find the Pearson correlation coefficient between X and Y
r_correlation <- cor(X, Y)
cat("Pearson correlation (r):", r_correlation, "\n")

## Square the Pearson correlation to get R2 or r squared
r_squared_from_correlation <- r_correlation^2
cat("R-squared from r^2:", r_squared_from_correlation, "\n")

## Check to see if R2 from the model matches r squared from the Pearson correlation
identical(round(r_squared_from_model, 6), round(r_squared_from_correlation, 6))
```
##2. We will use the â€˜mtcarsâ€™ dataset for this question. The dataset is already included in your R distribution. The dataset shows some of the characteristics of different cars.
```{r}
cars_dataset <- read.csv("C:\\Users\\ecrawle1\\OneDrive - Kent State University\\Desktop\\MS Business Analytics\\Business Analytics M25\\Assignment 3\\mtcars.csv", stringsAsFactors = FALSE)


str(cars_dataset)
```
##a) James wants to buy a car. He and his friend, Chris, have different opinions about the Horse Power (hp) of cars. James think the weight of a car (wt) can be used to estimate the Horse Power of the car while Chris thinks the fuel consumption expressed in Mile Per Gallon (mpg), is a better estimator of the (hp). Who do you think is right? Construct simple linear models using mtcars data to answer the question. (17% of total points)
```{r}
cars_dataset <- read.csv("C:\\Users\\ecrawle1\\OneDrive - Kent State University\\Desktop\\MS Business Analytics\\Business Analytics M25\\Assignment 3\\mtcars.csv", stringsAsFactors = FALSE)

str(cars_dataset)

# James's model: Predict hp using weight (wt)
model_james <- lm(hp ~ wt, data = cars_dataset)
summary(model_james)

# Chris's model: Predict hp using mpg
model_chris <- lm(hp ~ mpg, data = cars_dataset)
summary(model_chris)

# Compare R-squared values
r2_james <- summary(model_james)$r.squared
r2_chris <- summary(model_chris)$r.squared

cat("James's R-squared (hp ~ wt):", r2_james, "\n")
cat("Chris's R-squared (hp ~ mpg):", r2_chris, "\n")
```
```{r}
## After constructing two simple linear regression models using the mtcars dataset, we found that: 

## James's model predicting horsepower (hp) from weight (wt) had an R-squared of 0.4339.

## Chris's model predicting horsepower from miles per gallon (mpg) had a higher R-squared of 0.6024.

## This means that miles per gallon explains more of the variation in horsepower than the weight does. Therefore, Chris is correct â€” the miles per gallon of a car is a better predictor of horsepower than its weight.
```

##b) Build a model that uses the number of cylinders (cyl) and the mile per gallon (mpg) values of a car to predict the car Horse Power (hp). Using this model, what is the estimated Horse Power of a car with 4 cylinder and mpg of 22? (17% of total points)
```{r}
model_multi <- lm(hp ~ cyl + mpg, data = cars_dataset)

summary(model_multi)

new_car <- data.frame(cyl = 4, mpg = 22)

predicted_hp <- predict(model_multi, newdata = new_car)
cat("Estimated Horsepower:", predicted_hp, "\n")
```
```{r}
## From this model, the estimated Horse Power of a car with 4 cylinder and mpg of 22 is 88.94
## The R2 value is around 0.71 so it explains about 71.09% of the variance in horsepower (Y). But this is multiple regression (Y ~ X1 + X2), so R2 is not equal to the square of any one correlation, instead it reflects how all the predictors (cyl X1 and mpg X2) together explain hp Y
```

##3. For this question, we are going to use BostonHousing dataset. The dataset is in â€˜mlbenchâ€™ package, so we first need to instal the package, call the library and the load the dataset using the following commands.
##The dataset contains information about houses in different parts of Boston.
```{r}
install.packages("mlbench", repos = "https://cloud.r-project.org")
library(mlbench)
data(BostonHousing)
str(BostonHousing)
```
##a) Build a model to estimate the median value of owner-occupied homes (medv)based on the following variables: crime crate (crim), proportion of residential land zoned for lots over 25,000 sq.ft (zn), the local pupil-teacher ratio (ptratio) and whether the tract bounds Chas River(chas). Is this an accurate model? (Hint check R2 ) (8% of total points)
```{r}
model_boston <- lm(medv ~ crim + zn + ptratio + chas, data = BostonHousing)

summary(model_boston)
```
```{r}
## Multiple R2 = 0.3599 The model shows the value of around 0.36 for multiple r squared which explains about 36% of the variance in home values

## Adjusted R-squared = 0.3547 this value is Slightly lower than the mulitple R2, accounting for the number of predictors

## All predictors are statistically significant (p-values < 0.001) which means they all contribute to the prediction of medv

## This is considered a "moderately accurate" model because all the predictors are statistically significant, but maybe other variables would improve the predictive performance.
```

##b) Use the estimated coefficient to answer these questions?

##I. Imagine two houses that are identical in all aspects but one bounds the Chas River and the other does not. Which one is more expensive and by how much? (8% of total points)
```{r}
##The house that bounds the Charles River is estimated to be worth $4,584 more (rounded to the nearest dollar) than an identical house that does not, all else being equal.
##From the Coefficients, the estimated standard deviation for a house bound by the Charles River is estimated to be worth more :chas1        4.58393
```
```{r}
# Coefficient for chas1
chas_coef <- coef(model_boston)["chas1"]
chas_coef
```

##II. Imagine two houses that are identical in all aspects but in the neighborhood of one of them the pupil-teacher ratio is 15 and in the other one is 18. Which one is more expensive and by how much? (Golden Question: 4% extra)
```{r}
##The house in the neighborhood with a pupil-teacher ratio of 15 is estimated to be worth $4,481 more than the one in the neighborhood with a ratio of 18, all else being equal.

## The coefficient for ptratio is -1.49367 
## A higher ptratio means lower quality schools (larger class sizes), which reduces home value
## Calculation: medv = -1.49367 X (18-15)= -4.48101
```
```{r}
ptratio_coef <- coef(model_boston)["ptratio"]

ptratio_diff <- ptratio_coef * (18 - 15)
ptratio_diff
```
##c) Which of the variables are statistically important (i.e. related to the house price)? Hint: use the p-values of the coefficients to answer. (8% of total points)
```{r}
model_boston <- lm(medv ~ crim + zn + ptratio + chas, data = BostonHousing)

summary(model_boston)

```
```{r}
##If p-value < 0.05 â†’ the variable is statistically significant (strong evidence it's related to house price)
## All values : Crime rate (crim) 2.20e-10 ***, Zoning proportion (zn) 6.14e-06 ***, Pupil-teacher ratio (ptratio) < 2e-16, and Charles River proximity (chas) 0.000514 have a p-value less than 0.05 which means they all have statistically significant relationships with median home value in the BostonHousing dataset.

## The most statistically significant variable is Pupil-teacher ratio ptratio, with a p-value of < 2e-16, which is the smallest p-value among all predictors.
```
##d) Use the anova analysis and determine the order of importance of these four variables. (18% of total points)
```{r}
model_boston <- lm(medv ~ crim + zn + ptratio + chas, data = BostonHousing)

anova(model_boston)
```
```{r}
## To determine the order of importance among all variables using the ANOVA analysis, look at the sum of squares and the F values. Larger values have greater importance. 

## From the ANOVA analysis of this model, the variables are in order from most important predictor of medv to least: Crime rate (crim) SS: 6440.8 FV: 118.007, Pupil-teacher ratio (ptratio) SS:4709.5 FV:86.287, Proportion of zoned land (zn) SS:3554.3 FV: 65.122, Charles River dummy (chas) SS:667.2 FV:12.224

##This ranking is based on the Sum of Squares and F-values, which reflect how much unique variance each variable explains in the model. 
##This also demonstrates that using the P value as a measure of statistical significance, like question 3C, Sum of Squares and F-values give a stronger indication of each variableâ€™s relative importance in the model.
```

